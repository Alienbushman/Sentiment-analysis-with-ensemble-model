{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "#for first time using nltk\n",
    "# nltk.download ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits=10\n",
    "stop_words = stopwords.words('english') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n",
    "    txt = re.sub('[^a-zA-Z ]+', '', txt) #only allows for letters\n",
    "    txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stop_words ]) # stem & remove stop words\n",
    "    return txt\n",
    "\n",
    "def print_model_performance(target,predicted):\n",
    "    print('outcome of training')\n",
    "    print(classification_report( target,predicted))   #uncomment if you want to see full report \n",
    "    print('test average accuracy ',accuracy_score( target,predicted))\n",
    "    print(confusion_matrix( target,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(text, threshold=0.05, engl=True):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    if engl:\n",
    "        trans = text\n",
    "    else:\n",
    "        trans = translator.translate(text).text\n",
    "\n",
    "    score = analyser.polarity_scores(trans)\n",
    "    lb = score['compound']\n",
    "    return lb\n",
    "    if lb >= threshold:\n",
    "        return 1\n",
    "    elif (lb > -threshold) and (lb < threshold):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def train_test_split_features(train, test,train_feature, target_feature,vectorise):\n",
    "    y_train = train[target_feature]   \n",
    "    X_train = train[train_feature]\n",
    "    y_test = test[target_feature]   \n",
    "    X_test = test[train_feature]\n",
    "    feature_names=[]\n",
    "    if(vectorise):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1, 4)) # create Count vectorizer.\n",
    "        X_train = vect.fit(X_train).transform(X_train) # transform text_train  into a vector \n",
    "        X_test = vect.transform(X_test) \n",
    "        feature_names = vect.get_feature_names() # to return all words used in vectorizer\n",
    "  \n",
    "    return X_train, X_test, y_train, y_test, feature_names\n",
    "\n",
    "#get this working a bit better later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(dataset, test_run=False):\n",
    "    data_df=[]\n",
    "    if (dataset == 'redit_data'):\n",
    "        dataset_location=\"datasets/Twitter and Reddit Sentimental analysis Dataset/Twitter_Data.csv\"\n",
    "        text_variable='clean_text'\n",
    "        target_feature='category'\n",
    "        data_df = pd.read_csv(dataset_location) \n",
    "\n",
    "    if (dataset == 'financial'):\n",
    "        text_variable='clean_text'\n",
    "        target_feature='category'\n",
    "        dataset_location=\"datasets/Sentiment Analysis for Financial News/all-data.csv\"\n",
    "        data_df = pd.read_csv(dataset_location,encoding='ISO-8859-1',header='infer',)\n",
    "        data_df.columns = [ target_feature,text_variable]\n",
    "    if (dataset == 'us_airline'):\n",
    "        target_feature='airline_sentiment'\n",
    "        text_variable='text'\n",
    "        dataset_location='datasets/Twitter US Airline Sentiment/Tweets.csv'\n",
    "        data_df = pd.read_csv(dataset_location) \n",
    "        \n",
    "    data_df=data_df[[text_variable,target_feature]]\n",
    "    data_df.columns = [ 'clean_text','target']\n",
    "    data_df[text_variable] = data_df['clean_text'].astype(str)\n",
    "    data_df[text_variable] = data_df['clean_text'].apply(clean_text) \n",
    "#     df.cc.astype('category').cat.codes\n",
    "#     data_df['target']=data_df['target'].cat.codes\n",
    "    data_df['target'] = pd.factorize(data_df['target'])[0] \n",
    "    if test_run:\n",
    "        data_df,_,_ = quick_run(data_df)\n",
    "    data_df = data_df.dropna()\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return data_df\n",
    "\n",
    "# a list of all models used\n",
    "def all_models():\n",
    "    #Using the recomended classifiers\n",
    "    #https://arxiv.org/abs/1708.05070\n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestClassifier(n_estimators=500, max_features=0.25, criterion=\"entropy\")\n",
    "    SVM = SVC(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesClassifier(n_estimators=1000, max_features=\"log2\", criterion=\"entropy\")\n",
    "    LR = LogisticRegression(C=1.5,fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBClassifier()\n",
    "    CBC = CatBoostClassifier(silent=True)\n",
    "    light_gb = lgb.LGBMClassifier()\n",
    "    models=[(LR, \"linear_regression\"),(ETC, \"Extra_tree_classifier\"),(SVM, \"support_vector_classifier\"), (RFC, \"random_forest_classifier\"), (GBC, \"gradient_boosted_classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light_GBM\"), (CBC, \"catboost_classifier\")]\n",
    "    #this subset was selected due to runtime\n",
    "    models=[(LR, \"linear_regression\"), (GBC, \"gradient_boosted_classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light_GBM\")]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the relavent machine learning model\n",
    "def run_features(df, model, splits,features='clean_text',vectorise=True, predict_probability=False):\n",
    "    cv = KFold(n_splits=splits, random_state=42, shuffle=False)\n",
    "    full_prediciton=[]\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train, test = df.loc[train_index], df.loc[test_index]\n",
    "        X_train, X_test, y_train, y_test, feature_names=train_test_split_features(train,test,features,'target', vectorise)\n",
    "        model.fit(X_train, y_train)\n",
    "        if (predict_probability==True):\n",
    "            prediction = model.predict_proba(X_test)\n",
    "        else:\n",
    "            prediction = model.predict(X_test)\n",
    "        full_prediciton.append(prediction)\n",
    "    predictions=[]\n",
    "    for set_of_prediction in full_prediciton:\n",
    "        for predicted in set_of_prediction:\n",
    "            predictions.append(predicted)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick way to run though a dataset to confirm that everything works\n",
    "def quick_run(df):\n",
    "    train, test = train_test_split(df, test_size=0.99)\n",
    "    train, validation = train_test_split(train, test_size=0.125)\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_datasets=['redit_data','financial','us_airline']\n",
    "data_df=pull_data('us_airline',test_run=False)\n",
    "predict_probability=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 33.62941312789917 seconds ---\n",
      "gradient_boosted_classifier\n",
      "--- 97.80858898162842 seconds ---\n",
      "XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 47.79335379600525 seconds ---\n",
      "Light_GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 58.408225536346436 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_predicted_names=[]\n",
    "models=all_models()\n",
    "df_copy=data_df.copy()\n",
    "for model, name in models:\n",
    "    print(name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions=run_features(df_copy,model,splits,predict_probability=predict_probability)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    predicted_name=name+'_prediction'\n",
    "    negative_prob=[]\n",
    "    neutral_prob=[]\n",
    "    positive_prob=[]\n",
    "    if (predict_probability):\n",
    "        for neg, neut,pos in predictions:\n",
    "            negative_prob.append(neg)\n",
    "            neutral_prob.append(neut)\n",
    "            positive_prob.append(pos)\n",
    "        neg_predictions=predicted_name+'_neg'\n",
    "        neut_predictions=predicted_name+'_neut'\n",
    "        pos_predictions=predicted_name+'_pos'\n",
    "        df_copy[neg_predictions]=negative_prob\n",
    "        df_copy[neut_predictions]=neutral_prob\n",
    "        df_copy[pos_predictions]=positive_prob\n",
    "\n",
    "        model_predicted_names.append(neg_predictions)\n",
    "        model_predicted_names.append(neut_predictions)\n",
    "        model_predicted_names.append(pos_predictions)\n",
    "        negative_prob.clear()\n",
    "        neutral_prob.clear()\n",
    "        positive_prob.clear()\n",
    "    else:\n",
    "        df_copy[predicted_name]=predictions\n",
    "        model_predicted_names.append(predicted_name)\n",
    "\n",
    "        print_model_performance(df_copy['target'],predictions)\n",
    "        predictions.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['polarity']=df_copy['clean_text'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df_copy['subjectivity']=df_copy['clean_text'].apply(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "df_copy['vader_sentiment']=df_copy['clean_text'].apply(lambda tweet: sentiment_analyzer_scores(tweet))\n",
    "model_predicted_names.append('polarity')\n",
    "model_predicted_names.append('subjectivity')\n",
    "model_predicted_names.append('vader_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome of training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.59      0.63      3099\n",
      "           1       0.76      0.70      0.73      2363\n",
      "           2       0.85      0.91      0.88      9178\n",
      "\n",
      "    accuracy                           0.81     14640\n",
      "   macro avg       0.77      0.73      0.75     14640\n",
      "weighted avg       0.80      0.81      0.81     14640\n",
      "\n",
      "test average accuracy  0.8106557377049181\n",
      "[[1814  267 1018]\n",
      " [ 292 1663  408]\n",
      " [ 535  252 8391]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "predictions=run_features(df_copy,model,splits, features=model_predicted_names,vectorise=False)\n",
    "print_model_performance(df_copy['target'],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>linear_regression_prediction_neg</th>\n",
       "      <th>linear_regression_prediction_neut</th>\n",
       "      <th>linear_regression_prediction_pos</th>\n",
       "      <th>gradient_boosted_classifier_prediction_neg</th>\n",
       "      <th>gradient_boosted_classifier_prediction_neut</th>\n",
       "      <th>gradient_boosted_classifier_prediction_pos</th>\n",
       "      <th>XGBoost_prediction_neg</th>\n",
       "      <th>XGBoost_prediction_neut</th>\n",
       "      <th>XGBoost_prediction_pos</th>\n",
       "      <th>Light_GBM_prediction_neg</th>\n",
       "      <th>Light_GBM_prediction_neut</th>\n",
       "      <th>Light_GBM_prediction_pos</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "      <td>0.273156</td>\n",
       "      <td>0.025889</td>\n",
       "      <td>0.700955</td>\n",
       "      <td>0.196817</td>\n",
       "      <td>0.068586</td>\n",
       "      <td>0.734597</td>\n",
       "      <td>0.509426</td>\n",
       "      <td>0.039224</td>\n",
       "      <td>0.451350</td>\n",
       "      <td>0.543241</td>\n",
       "      <td>0.030542</td>\n",
       "      <td>0.426217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plu youv ad commerci experi tacki</td>\n",
       "      <td>0.166651</td>\n",
       "      <td>0.129347</td>\n",
       "      <td>0.704002</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.864161</td>\n",
       "      <td>0.118286</td>\n",
       "      <td>0.138499</td>\n",
       "      <td>0.743214</td>\n",
       "      <td>0.157750</td>\n",
       "      <td>0.125022</td>\n",
       "      <td>0.717228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica didnt today must mean need take ...</td>\n",
       "      <td>0.332415</td>\n",
       "      <td>0.031368</td>\n",
       "      <td>0.636218</td>\n",
       "      <td>0.322555</td>\n",
       "      <td>0.057849</td>\n",
       "      <td>0.619596</td>\n",
       "      <td>0.446112</td>\n",
       "      <td>0.046278</td>\n",
       "      <td>0.507609</td>\n",
       "      <td>0.387729</td>\n",
       "      <td>0.044367</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>-0.390625</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2</td>\n",
       "      <td>virginamerica realli aggress blast obnoxi ente...</td>\n",
       "      <td>0.081795</td>\n",
       "      <td>0.084485</td>\n",
       "      <td>0.833720</td>\n",
       "      <td>0.062889</td>\n",
       "      <td>0.036372</td>\n",
       "      <td>0.900738</td>\n",
       "      <td>0.136733</td>\n",
       "      <td>0.116503</td>\n",
       "      <td>0.746763</td>\n",
       "      <td>0.088637</td>\n",
       "      <td>0.074582</td>\n",
       "      <td>0.836781</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>-0.2716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2</td>\n",
       "      <td>virginamerica realli big bad thing</td>\n",
       "      <td>0.074216</td>\n",
       "      <td>0.095449</td>\n",
       "      <td>0.830335</td>\n",
       "      <td>0.101453</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>0.863194</td>\n",
       "      <td>0.098442</td>\n",
       "      <td>0.149364</td>\n",
       "      <td>0.752194</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.128055</td>\n",
       "      <td>0.797739</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>-0.5829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>1</td>\n",
       "      <td>americanair thank got differ flight chicago</td>\n",
       "      <td>0.401143</td>\n",
       "      <td>0.505183</td>\n",
       "      <td>0.093674</td>\n",
       "      <td>0.065390</td>\n",
       "      <td>0.839596</td>\n",
       "      <td>0.095014</td>\n",
       "      <td>0.104683</td>\n",
       "      <td>0.852461</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.864741</td>\n",
       "      <td>0.030945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>2</td>\n",
       "      <td>americanair leav  minut late flight warn commu...</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.993320</td>\n",
       "      <td>0.031886</td>\n",
       "      <td>0.011740</td>\n",
       "      <td>0.956374</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.990860</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.4043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair pleas bring american airlin blackb...</td>\n",
       "      <td>0.682025</td>\n",
       "      <td>0.139579</td>\n",
       "      <td>0.178396</td>\n",
       "      <td>0.585465</td>\n",
       "      <td>0.089334</td>\n",
       "      <td>0.325201</td>\n",
       "      <td>0.481403</td>\n",
       "      <td>0.107839</td>\n",
       "      <td>0.410758</td>\n",
       "      <td>0.704197</td>\n",
       "      <td>0.086921</td>\n",
       "      <td>0.208882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>2</td>\n",
       "      <td>americanair money chang flight dont answer pho...</td>\n",
       "      <td>0.122450</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.854069</td>\n",
       "      <td>0.152957</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.791339</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.031588</td>\n",
       "      <td>0.851561</td>\n",
       "      <td>0.059958</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.929621</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>0</td>\n",
       "      <td>americanair  ppl need  know mani seat next fli...</td>\n",
       "      <td>0.223507</td>\n",
       "      <td>0.056199</td>\n",
       "      <td>0.720294</td>\n",
       "      <td>0.296468</td>\n",
       "      <td>0.086688</td>\n",
       "      <td>0.616844</td>\n",
       "      <td>0.438012</td>\n",
       "      <td>0.048209</td>\n",
       "      <td>0.513779</td>\n",
       "      <td>0.464869</td>\n",
       "      <td>0.103497</td>\n",
       "      <td>0.431634</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  target  \\\n",
       "0                    @VirginAmerica What @dhepburn said.       0   \n",
       "1      @VirginAmerica plus you've added commercials t...       1   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...       0   \n",
       "3      @VirginAmerica it's really aggressive to blast...       2   \n",
       "4      @VirginAmerica and it's a really big bad thing...       2   \n",
       "...                                                  ...     ...   \n",
       "14635  @AmericanAir thank you we got on a different f...       1   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...       2   \n",
       "14637  @AmericanAir Please bring American Airlines to...       0   \n",
       "14638  @AmericanAir you have my money, you change my ...       2   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...       0   \n",
       "\n",
       "                                                    text  \\\n",
       "0                            virginamerica dhepburn said   \n",
       "1        virginamerica plu youv ad commerci experi tacki   \n",
       "2      virginamerica didnt today must mean need take ...   \n",
       "3      virginamerica realli aggress blast obnoxi ente...   \n",
       "4                     virginamerica realli big bad thing   \n",
       "...                                                  ...   \n",
       "14635        americanair thank got differ flight chicago   \n",
       "14636  americanair leav  minut late flight warn commu...   \n",
       "14637  americanair pleas bring american airlin blackb...   \n",
       "14638  americanair money chang flight dont answer pho...   \n",
       "14639  americanair  ppl need  know mani seat next fli...   \n",
       "\n",
       "       linear_regression_prediction_neg  linear_regression_prediction_neut  \\\n",
       "0                              0.273156                           0.025889   \n",
       "1                              0.166651                           0.129347   \n",
       "2                              0.332415                           0.031368   \n",
       "3                              0.081795                           0.084485   \n",
       "4                              0.074216                           0.095449   \n",
       "...                                 ...                                ...   \n",
       "14635                          0.401143                           0.505183   \n",
       "14636                          0.004824                           0.001856   \n",
       "14637                          0.682025                           0.139579   \n",
       "14638                          0.122450                           0.023481   \n",
       "14639                          0.223507                           0.056199   \n",
       "\n",
       "       linear_regression_prediction_pos  \\\n",
       "0                              0.700955   \n",
       "1                              0.704002   \n",
       "2                              0.636218   \n",
       "3                              0.833720   \n",
       "4                              0.830335   \n",
       "...                                 ...   \n",
       "14635                          0.093674   \n",
       "14636                          0.993320   \n",
       "14637                          0.178396   \n",
       "14638                          0.854069   \n",
       "14639                          0.720294   \n",
       "\n",
       "       gradient_boosted_classifier_prediction_neg  \\\n",
       "0                                        0.196817   \n",
       "1                                        0.093975   \n",
       "2                                        0.322555   \n",
       "3                                        0.062889   \n",
       "4                                        0.101453   \n",
       "...                                           ...   \n",
       "14635                                    0.065390   \n",
       "14636                                    0.031886   \n",
       "14637                                    0.585465   \n",
       "14638                                    0.152957   \n",
       "14639                                    0.296468   \n",
       "\n",
       "       gradient_boosted_classifier_prediction_neut  \\\n",
       "0                                         0.068586   \n",
       "1                                         0.041864   \n",
       "2                                         0.057849   \n",
       "3                                         0.036372   \n",
       "4                                         0.035354   \n",
       "...                                            ...   \n",
       "14635                                     0.839596   \n",
       "14636                                     0.011740   \n",
       "14637                                     0.089334   \n",
       "14638                                     0.055705   \n",
       "14639                                     0.086688   \n",
       "\n",
       "       gradient_boosted_classifier_prediction_pos  XGBoost_prediction_neg  \\\n",
       "0                                        0.734597                0.509426   \n",
       "1                                        0.864161                0.118286   \n",
       "2                                        0.619596                0.446112   \n",
       "3                                        0.900738                0.136733   \n",
       "4                                        0.863194                0.098442   \n",
       "...                                           ...                     ...   \n",
       "14635                                    0.095014                0.104683   \n",
       "14636                                    0.956374                0.004014   \n",
       "14637                                    0.325201                0.481403   \n",
       "14638                                    0.791339                0.116851   \n",
       "14639                                    0.616844                0.438012   \n",
       "\n",
       "       XGBoost_prediction_neut  XGBoost_prediction_pos  \\\n",
       "0                     0.039224                0.451350   \n",
       "1                     0.138499                0.743214   \n",
       "2                     0.046278                0.507609   \n",
       "3                     0.116503                0.746763   \n",
       "4                     0.149364                0.752194   \n",
       "...                        ...                     ...   \n",
       "14635                 0.852461                0.042855   \n",
       "14636                 0.005126                0.990860   \n",
       "14637                 0.107839                0.410758   \n",
       "14638                 0.031588                0.851561   \n",
       "14639                 0.048209                0.513779   \n",
       "\n",
       "       Light_GBM_prediction_neg  Light_GBM_prediction_neut  \\\n",
       "0                      0.543241                   0.030542   \n",
       "1                      0.157750                   0.125022   \n",
       "2                      0.387729                   0.044367   \n",
       "3                      0.088637                   0.074582   \n",
       "4                      0.074206                   0.128055   \n",
       "...                         ...                        ...   \n",
       "14635                  0.104315                   0.864741   \n",
       "14636                  0.005531                   0.004408   \n",
       "14637                  0.704197                   0.086921   \n",
       "14638                  0.059958                   0.010421   \n",
       "14639                  0.464869                   0.103497   \n",
       "\n",
       "       Light_GBM_prediction_pos  polarity  subjectivity  vader_sentiment  \n",
       "0                      0.426217  0.000000      0.000000           0.0000  \n",
       "1                      0.717228  0.000000      0.000000           0.0000  \n",
       "2                      0.567905 -0.390625      0.687500           0.0000  \n",
       "3                      0.836781  0.006250      0.350000          -0.2716  \n",
       "4                      0.797739 -0.350000      0.383333          -0.5829  \n",
       "...                         ...       ...           ...              ...  \n",
       "14635                  0.030945  0.000000      0.600000           0.3612  \n",
       "14636                  0.990062 -0.300000      0.600000          -0.4043  \n",
       "14637                  0.208882  0.000000      0.000000           0.3182  \n",
       "14638                  0.929621 -0.125000      0.375000           0.5027  \n",
       "14639                  0.431634  0.166667      0.166667           0.0772  \n",
       "\n",
       "[14640 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_regression_prediction_neg',\n",
       " 'linear_regression_prediction_neut',\n",
       " 'linear_regression_prediction_pos',\n",
       " 'gradient_boosted_classifier_prediction_neg',\n",
       " 'gradient_boosted_classifier_prediction_neut',\n",
       " 'gradient_boosted_classifier_prediction_pos',\n",
       " 'XGBoost_prediction_neg',\n",
       " 'XGBoost_prediction_neut',\n",
       " 'XGBoost_prediction_pos',\n",
       " 'Light_GBM_prediction_neg',\n",
       " 'Light_GBM_prediction_neut',\n",
       " 'Light_GBM_prediction_pos',\n",
       " 'polarity',\n",
       " 'subjectivity',\n",
       " 'vader_sentiment']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predicted_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
