{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import string\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "#for first time using nltk\n",
    "# nltk.download ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_variable='clean_text'\n",
    "target_feature='category'\n",
    "splits=10\n",
    "stop_words = stopwords.words('english') \n",
    "dataset_location=\"datasets/Twitter and Reddit Sentimental analysis Dataset/Twitter_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n",
    "    txt = re.sub('[^a-zA-Z ]+', '', txt) #only allows for letters\n",
    "    txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stop_words ]) # stem & remove stop words\n",
    "    return txt\n",
    "\n",
    "def print_model_performance(target,predicted):\n",
    "#     training_sample = model.predict(X_train)\n",
    "#     testing_sample = model.predict(X_test)\n",
    "#     print('training ')\n",
    "#     print('train accuracy ',accuracy_score(training_sample, y_train))\n",
    "#     print('\\n testing  ')\n",
    "#     print('test average accuracy ',accuracy_score(testing_sample, y_test))\n",
    "#     print(confusion_matrix( y_test,testing_sample))\n",
    "    print('outcome of training')\n",
    "    print(classification_report( target,predicted))   #uncomment if you want to see full report \n",
    "    print('test average accuracy ',accuracy_score( target,predicted))\n",
    "    print(confusion_matrix( target,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(text, threshold=0.05, engl=True):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    if engl:\n",
    "        trans = text\n",
    "    else:\n",
    "        trans = translator.translate(text).text\n",
    "    score = analyser.polarity_scores(trans)\n",
    "    lb = score['compound']\n",
    "    return lb\n",
    "    if lb >= threshold:\n",
    "        return 1\n",
    "    elif (lb > -threshold) and (lb < threshold):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def train_test_split_features(train, test,train_feature, target_feature,vectorise):\n",
    "    y_train = train[target_feature]   \n",
    "    X_train = train[train_feature]\n",
    "    y_test = test[target_feature]   \n",
    "    X_test = test[train_feature]\n",
    "    feature_names=[]\n",
    "    if(vectorise):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1, 4)) # create Count vectorizer.\n",
    "        X_train = vect.fit(X_train).transform(X_train) # transform text_train  into a vector \n",
    "        X_test = vect.transform(X_test) \n",
    "        feature_names = vect.get_feature_names() # to return all words used in vectorizer\n",
    "  \n",
    "    return X_train, X_test, y_train, y_test, feature_names\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "#get this working a bit better later\n",
    "def all_models():\n",
    "    #Using the recomended classifiers\n",
    "    #https://arxiv.org/abs/1708.05070\n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestClassifier(n_estimators=500, max_features=0.25, criterion=\"entropy\")\n",
    "    SVM = SVC(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesClassifier(n_estimators=1000, max_features=\"log2\", criterion=\"entropy\")\n",
    "    LR = LogisticRegression(C=1.5,fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBClassifier()\n",
    "    CBC = CatBoostClassifier(silent=True)\n",
    "    light_gb = lgb.LGBMClassifier()\n",
    "#     models=[(LR, \"linear_regression\"),(ETC, \"Extra_tree_classifier\"),(SVM, \"support_vector_classifier\"), (RFC, \"random_forest_classifier\"), (GBC, \"gradient_boosted_classifier\"),\n",
    "#              (XGC, \"XGBoost\"),(light_gb,\"Light_GBM\"), (CBC, \"catboost_classifier\")]\n",
    "    models=[(LR, \"linear_regression\"),(SVM, \"support_vector_classifier\"), \n",
    "              (CBC, \"catboost_classifier\"),(XGC, \"XGBoost\")]\n",
    "#      (RFC, \"random_forest_classifier\"),(ETC, \"Extra_tree_classifier\"),(GBC, \"gradient_boosted_classifier\"),\n",
    "#                (CBC, \"catboost_classifier\")]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run(df):\n",
    "    train, test = train_test_split(df, test_size=0.99)\n",
    "    train, validation = train_test_split(train, test_size=0.125)\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dataset_location) \n",
    "\n",
    "data_df[text_variable] = data_df[text_variable].astype(str)\n",
    "data_df[text_variable] = data_df[text_variable].apply(clean_text)  \n",
    "data_df = data_df.dropna()\n",
    "data_df,_,_=quick_run(data_df)\n",
    "data_df=data_df.reset_index(drop=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_features(df, model, splits,features=text_variable,vectorise=True):\n",
    "    cv = KFold(n_splits=splits, random_state=42, shuffle=False)\n",
    "    full_prediciton=[]\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        train, test = df.loc[train_index], df.loc[test_index]\n",
    "\n",
    "        X_train, X_test, y_train, y_test, feature_names=train_test_split_features(train,test,text_variable,target_feature, True)\n",
    "#         model = LogisticRegressionCV()\n",
    "        #to run faster\n",
    "#         model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        #print_model_performance(lgstc, X_train, X_test, y_train, y_test)\n",
    "        prediction = model.predict(X_test)\n",
    "        full_prediciton.append(prediction)\n",
    "    predictions=[]\n",
    "\n",
    "    for set_of_prediction in full_prediciton:\n",
    "        for predicted in set_of_prediction:\n",
    "            predictions.append(predicted)\n",
    "    return predictions\n",
    "# to_dataset=data_df.copy()\n",
    "\n",
    "\n",
    "model_predicted_names=[]\n",
    "models=all_models()\n",
    "df_copy=data_df.copy()\n",
    "for model, name in models:\n",
    "    print(name)\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    predictions=run_features(df_copy,model,splits)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    predicted_name=name+'_prediction'\n",
    "    df_copy[predicted_name]=predictions\n",
    "    model_predicted_names.append(predicted_name)\n",
    "    \n",
    "    print_model_performance(df_copy[target_feature],predictions)\n",
    "    predictions.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(text, threshold=0.05, engl=True):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    if engl:\n",
    "        trans = text\n",
    "    else:\n",
    "        trans = translator.translate(text).text\n",
    "\n",
    "    score = analyser.polarity_scores(trans)\n",
    "    lb = score['compound']\n",
    "    return lb\n",
    "    if lb >= threshold:\n",
    "        return 1\n",
    "    elif (lb > -threshold) and (lb < threshold):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['polarity']=df_copy[text_variable].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df_copy['subjectivity']=df_copy[text_variable].apply(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "df_copy['vader_sentiment']=df_copy[text_variable].apply(lambda tweet: sentiment_analyzer_scores(tweet))\n",
    "model_predicted_names.append('polarity')\n",
    "model_predicted_names.append('subjectivity')\n",
    "model_predicted_names.append('vader_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "predictions=run_features(df_copy,model,split, features=model_predicted_names,vectorise=False)\n",
    "print_model_performance(df_copy[target_feature],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
